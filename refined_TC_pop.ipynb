{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "refined_TC_pop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tYP9zSubkkI",
        "colab_type": "code",
        "outputId": "e2039f7b-e12a-4c4e-b3c1-06d59ec87723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "!apt-get install libgeos-3.5.0\n",
        "!apt-get install libgeos-dev\n",
        "!pip install https://github.com/matplotlib/basemap/archive/master.zip\n",
        "!pip install pyproj==1.9.6\n",
        "\n",
        "!pip install rasterio\n",
        "\n",
        "!apt-get install libproj-dev proj-data proj-bin\n",
        "!apt-get install libgeos-dev\n",
        "!pip install cython\n",
        "!pip install cartopy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package libgeos-3.5.0\n",
            "E: Couldn't find any package by glob 'libgeos-3.5.0'\n",
            "E: Couldn't find any package by regex 'libgeos-3.5.0'\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  libgdal-doc\n",
            "The following NEW packages will be installed:\n",
            "  libgeos-dev\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 73.1 kB of archives.\n",
            "After this operation, 486 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgeos-dev amd64 3.6.2-1build2 [73.1 kB]\n",
            "Fetched 73.1 kB in 0s (158 kB/s)\n",
            "Selecting previously unselected package libgeos-dev.\n",
            "(Reading database ... 132684 files and directories currently installed.)\n",
            "Preparing to unpack .../libgeos-dev_3.6.2-1build2_amd64.deb ...\n",
            "Unpacking libgeos-dev (3.6.2-1build2) ...\n",
            "Setting up libgeos-dev (3.6.2-1build2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting https://github.com/matplotlib/basemap/archive/master.zip\n",
            "\u001b[?25l  Downloading https://github.com/matplotlib/basemap/archive/master.zip (133.1MB)\n",
            "\u001b[K     |████████████████████████████████| 133.1MB 15kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib!=3.0.1,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from basemap==1.2.1) (3.1.1)\n",
            "Requirement already satisfied: numpy>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from basemap==1.2.1) (1.17.3)\n",
            "Collecting pyproj>=1.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/45/ea3ae47cd95fcb9f8631c290d00377196aa5f409792b49e4a8a20a3588f5/pyproj-2.4.1-cp36-cp36m-manylinux2010_x86_64.whl (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 3.4MB/s \n",
            "\u001b[?25hCollecting pyshp>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/16/3bf15aa864fb77845fab8007eda22c2bd67bd6c1fd13496df452c8c43621/pyshp-2.1.0.tar.gz (215kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 44.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from basemap==1.2.1) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.1,>=1.0.0->basemap==1.2.1) (2.6.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.1,>=1.0.0->basemap==1.2.1) (2.4.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.1,>=1.0.0->basemap==1.2.1) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.1,>=1.0.0->basemap==1.2.1) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.1,>=1.0.0->basemap==1.2.1) (41.4.0)\n",
            "Building wheels for collected packages: basemap, pyshp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tWq6-oSMz_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql9faYy2cVsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import rasterio\n",
        "import gdal\n",
        "\n",
        "from pyproj import Proj, transform\n",
        "from scipy.io import loadmat\n",
        " \n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.io.img_tiles as cimgt\n",
        "from mpl_toolkits.basemap import Basemap\n",
        "\n",
        "from numpy import meshgrid\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJWoF2A3ckP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dset_gtiff = \"/content/drive/My Drive/Data/gpw_v4_population_count_rev11_2020_2pt5_min.tif\"\n",
        "gdal.AllRegister()\n",
        "global ds\n",
        "ds = gdal.Open(dset_gtiff)\n",
        "band = ds.GetRasterBand(1)\n",
        "arr = band.ReadAsArray()\n",
        "ds_size = arr.shape\n",
        "if band.GetMinimum() is None or band.GetMaximum() is None:\n",
        "   band.ComputeStatistics(0)\n",
        "      \n",
        "print (\"[ NO DATA VALUE ] = \", band.GetNoDataValue()) # none\n",
        "print (\"[ MIN ] = \", band.GetMinimum())\n",
        "print (\"[ MAX ] = \", band.GetMaximum())\n",
        "print (ds_size)\n",
        "\n",
        "ds.GetProjection()\n",
        "dataGTIFF = ds.ReadAsArray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIgwlk2Fc9X6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### From Documentation\n",
        "The population count data are divided by the land area data to create the population density rasters for each respective year. The pixels used in GPWv4 are quadrilaterals and therefore the area of each pixel decreases with increasing latitude. Additionally, the pixels in the land area data that include water reflect the net land area (total area of pixel – area of water within the pixel). This means that the population density data cannot be multiplied by a fixed land area size to estimate population totals without introducing error in some or all of the pixels. Multiplying the population density values by pixel area will overestimate population in pixels that contain water. If a fixed value for pixel area is used, it will also over or underestimate population totals for pixels that are all land as the area of these pixels varies by latitude. The population density data are best used to determine statistics (minimum, maximum, mean, etc.) within varying geographies. Users who need population totals should use the GPWv4 population count data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF68heXUdX8G",
        "colab_type": "text"
      },
      "source": [
        "### Visualization on the Projection for People-Hours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acYxoxYb3Dbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gather_data(filter_cat = False, knots = True, category = 5, include_gt = True): \n",
        "  saffir_simpson_scale = {}\n",
        "  if knots:\n",
        "    saffir_simpson_scale = {0: lambda x: 0 <= x < 65, 1: lambda x: 64 <= x < 83, #NEEDS TO BE IN KNOTS\n",
        "                          2: lambda x: 83 <= x < 96, 3: lambda x: 96 <= x < 113,\n",
        "                          4: lambda x: 113 <= x < 136, 5: lambda x: x >= 136}\n",
        "  else: \n",
        "    saffir_simpson_scale = {0: lambda x: 0 <= x < 119, 1: lambda x: 119 <= x < 154,\n",
        "                          2: lambda x: 154 <= x < 178, 3: lambda x: 178 <= x < 209,\n",
        "                          4: lambda x: 209 <= x < 252, 5: lambda x: x >= 252}\n",
        "  all_data = [loadmat('/content/drive/My Drive/Data/TrackData_RMM_phase' + str(i + 1) + '.mat') for i in range(8)]\n",
        "  eight_phase = []\n",
        "  for dset in all_data:\n",
        "    dset_lat = [row for row in dset['latstore']]\n",
        "    dset_long = [row for row in dset['longstore']]\n",
        "    dset_vel = [row for row in dset['vstore']]\n",
        "    if not filter_cat:\n",
        "      hur_path = [zip(dset_lat[a], dset_long[a]) for a in range(4000)]\n",
        "    else:\n",
        "      all_paths = [zip(dset_lat[a], dset_long[a]) for a in range(4000)]\n",
        "      hur_path = []\n",
        "      for p in range(len(all_paths)):\n",
        "        hurricane = list(all_paths[p])\n",
        "        path = []\n",
        "        for i in range(len(hurricane)):\n",
        "          if hurricane[i] == (0.0,0.0):\n",
        "            path.append((float('nan'), float('nan')))\n",
        "          elif include_gt and any(saffir_simpson_scale[k](dset_vel[p][i]) for k in range(category, len(saffir_simpson_scale))):\n",
        "              path.append(hurricane[i])\n",
        "          elif saffir_simpson_scale[category](dset_vel[p][i]) :\n",
        "            path.append(hurricane[i])\n",
        "          else:\n",
        "            path.append((float('nan'),float('nan')))\n",
        "        hur_path.append(path)\n",
        "    eight_phase.append(hur_path)\n",
        "  return eight_phase\n",
        "\n",
        "def refine_data(data_set):\n",
        "  new_set = [[] for i in range(8)]\n",
        "  for phase in range(len(data_set)):\n",
        "    for hurricane in data_set[phase]:\n",
        "      new_set[phase].extend(split_hurricane(hurricane))\n",
        "  return new_set\n",
        "  \n",
        "def split_hurricane(path):\n",
        "  hurricanes = [[]]\n",
        "  for i in range(len(path)):\n",
        "    if not np.isnan(path[i][0]) and not np.isnan(path[i][1]):\n",
        "      hurricanes[-1].append((path[i][0], path[i][1]))\n",
        "    elif not np.isnan(path[i - 1][0]) and np.isnan(path[i][0]):\n",
        "      hurricanes.append([])\n",
        "  return hurricanes\n",
        "\n",
        "def transform_coordinate(j, i, padfTransform):\n",
        "  Yp = padfTransform[0] + i*padfTransform[1] + i*padfTransform[2];\n",
        "  Xp = padfTransform[3] + j*padfTransform[4] + j*padfTransform[5];\n",
        "  if Yp < 0:\n",
        "    Yp += 360\n",
        "  if Yp > 360:\n",
        "    Yp -= 360\n",
        "  return (Xp, Yp)\n",
        "\n",
        "def population_binning(data, width = 1.25):\n",
        "  pop_bins = np.zeros((int(90/width), int(280/width)))\n",
        "  for i in range(len(data) // 2):\n",
        "    for j in range(len(data[i])):\n",
        "      coords = transform_coordinate(i, j, ds.GetGeoTransform())\n",
        "      try:\n",
        "        if (0 <= coords[0] <= 90) and (0 <= coords[1] <= 280) and data[i][j] >= 1:\n",
        "          pop_bins[int(coords[0] / width)][int(coords[1] / width)] += round(data[i][j])\n",
        "      except:\n",
        "        print(coords)\n",
        "  return pop_bins\n",
        "\n",
        "def check_same_bin(x1, y1, x2, y2, width):\n",
        "  return int(x1/width) == int(x2/width) and int(y1/width) == int(y2/width) \n",
        "\n",
        "def hour_binning(hurr_phases, width = 1.25):\n",
        "  all_data = []\n",
        "  for phase in hurr_phases:\n",
        "    hour_bins = np.zeros((int(90/width), int(280/width))) #setting up a 2d-array for binning.\n",
        "    for hurricane in phase:\n",
        "      hurricane = list(hurricane)\n",
        "      hurricane = [coord for coord in hurricane if coord[0] != 0. and coord[1] != 0.]\n",
        "      for i in range(len(hurricane[1:])):\n",
        "        prev = hurricane[i-1]\n",
        "        if check_same_bin(prev[0], prev[1], hurricane[i][0], hurricane[i][1], width):\n",
        "          hour_bins[int(hurricane[i][0]/width)][int(hurricane[i][1] / width)] += 1\n",
        "        else:\n",
        "          split_hour(hour_bins, int(prev[0]/width), int(prev[1]/width), int(hurricane[i][0]/width), int(hurricane[i][1]/width))\n",
        "    all_data.append(hour_bins)\n",
        "  all_data = np.array([[[float('nan') if num < 1 else num for num in row] for row in data] for data in all_data])\n",
        "  return all_data\n",
        "\n",
        "def split_hour(hurr_phases, coord1x, coord1y, coord2x, coord2y):\n",
        "  dx = coord2x - coord1x\n",
        "  dy = coord2y - coord1y\n",
        "  increment = 1 /(abs(dx) + abs(dy))\n",
        "  incx, incy = 0,0\n",
        "  hurr_phases[coord1x][coord1y] += increment\n",
        "  while(dx != 0 or dy != 0):\n",
        "      if(abs(dx) == abs(dy)):\n",
        "        while(dx != 0 and dy !=0):\n",
        "          incx += 1 if dx > 0 else -1\n",
        "          incy += 1 if dy > 0 else -1\n",
        "          hurr_phases[coord1x+incx][coord1y+incy] += increment\n",
        "          dx -= 1 if dx > 0 else -1\n",
        "          dy -= 1 if dy > 0 else -1\n",
        "      else:\n",
        "        if(abs(dx) > abs(dy)):\n",
        "          incx += 1 if dx > 0 else -1\n",
        "          dx -= 1 if dx > 0 else -1\n",
        "          hurr_phases[coord1x+incx][coord1y+incy] += increment\n",
        "        elif (abs(dy) > abs(dx)):\n",
        "          incy += 1 if dy > 0 else -1\n",
        "          dy -= 1 if dy > 0 else -1\n",
        "          hurr_phases[coord1x+incx][coord1y+incy] += increment\n",
        "\n",
        "def popxhours(population, hour_set):\n",
        "  pop_hour_bins = [population*phase for phase in hour_set]\n",
        "  pop_hour_bins = [[[float('nan') if np.isnan(num) or num <= 1 else num for num in row] for row in data] for data in pop_hour_bins]\n",
        "  return np.array(pop_hour_bins)\n",
        "\n",
        "def popxhours_anomaly(pop_hour_bins, percentage = False): \n",
        "  sum_pop_hours = np.zeros(pop_hour_bins.shape[1:])\n",
        "  phases_participated = np.zeros(pop_hour_bins.shape[1:]) #Need to get the number of phases each bin has been in\n",
        "  for i in range(sum_pop_hours.shape[0]):\n",
        "    for j in range(sum_pop_hours.shape[1]):\n",
        "      vals = [phase[i,j] for phase in pop_hour_bins if not np.isnan(phase[i,j])]\n",
        "      sum_pop_hours[i,j] = np.nansum(vals)\n",
        "      phases_participated[i,j] = len(vals)\n",
        "  sum_pop_hours = np.array([[sum_pop_hours[i,j] / phases_participated[i,j] if phases_participated[i,j] > 0 else float('nan') for j in range(len(sum_pop_hours[i]))]for i in range(sum_pop_hours.shape[0])])\n",
        "  all_data = []\n",
        "  \n",
        "  for phase in pop_hour_bins:\n",
        "    phase_data = np.zeros(pop_hour_bins.shape[1:])\n",
        "    for i in range(phase_data.shape[0]):\n",
        "      for j in range(phase_data.shape[1]):\n",
        "        if not percentage:\n",
        "          phase_data[i,j] = float('nan') if np.isnan(sum_pop_hours[i,j]) else phase[i,j] - sum_pop_hours[i,j]\n",
        "        elif percentage and phases_participated[i,j] == 8:\n",
        "          phase_data[i,j] = float('nan') if np.isnan(sum_pop_hours[i,j]) else phase[i,j] - sum_pop_hours[i,j]\n",
        "          phase_data[i,j] = phase_data[i,j] * 100 /sum_pop_hours[i,j]\n",
        "        else:\n",
        "          phase_data[i,j] = float('nan')\n",
        "    all_data.append(phase_data)\n",
        "  return all_data\n",
        "\n",
        "def make_nans(threeDarr):\n",
        "  return [[[float('nan') if np.isnan(num) else num for num in row] for row in data] for data in threeDarr]\n",
        "  \n",
        "def create_maps(plot_labels, width = 1.25):\n",
        "  arr_with_nans = make_nans(plot_labels['data'])\n",
        "  y = np.arange(0,90, width)\n",
        "  x = np.arange(0,280,width)\n",
        "  for i in range(len(arr_with_nans)):\n",
        "    fig = plt.figure(figsize = (10, 8))\n",
        "    hm = Basemap(projection='lcc', resolution='c',\n",
        "            width=21E6, height=10E6, \n",
        "            lat_0=35, lon_0=145)\n",
        "    hm.shadedrelief(scale = 0.125)\n",
        "    parallels = np.arange(-90, 91, 10)\n",
        "    hm.drawparallels(parallels, labels = [True, False, False, False])\n",
        "    meridians = np.arange(0, 361,10)\n",
        "    hm.drawmeridians(meridians, lables = [False, False, True, True])\n",
        "    xx,yy = meshgrid(x,y) #the colormap and colorbar will be different depending on anomaly's value\n",
        "    hm.pcolormesh(xx,yy, arr_with_nans[i], latlon=True, cmap= plot_labels['cmapscheme'])\n",
        "    hm.drawcoastlines()\n",
        "    plt.clim(plot_labels['cmapmin'], plot_labels['cmapmax'])\n",
        "    plt.colorbar(label= plot_labels['cmaplabel'])\n",
        "    plt.title(plot_labels['label'] + ' - Phase %d' %(i+ 1))\n",
        "  \n",
        "def data_dict(data = None, label=None, xaxis=None, yaxis=None, cmapscheme=None, cmapmin=None, cmapmax=None, cmaplabel=None):\n",
        "  ps = {}\n",
        "  ps['data'] = data\n",
        "  ps['label'] = label\n",
        "  ps['xaxis'] = xaxis\n",
        "  ps['yaxis'] = yaxis\n",
        "  ps['cmapscheme'] = cmapscheme\n",
        "  ps['cmapmin'] = cmapmin\n",
        "  ps['cmapmax'] = cmapmax\n",
        "  ps['cmaplabel'] = cmaplabel\n",
        "  return ps\n",
        "\n",
        "def create_maps_subplotted(population_reference, hour_exposure, hour_anomalies, pop_hours, pop_hours_anomaly, pop_hours_diff, width = 1.25):\n",
        "  y = np.arange(0,90, width)\n",
        "  x = np.arange(0,280,width)\n",
        "  xx,yy = meshgrid(x,y)\n",
        "  parallels = np.arange(-90, 91, 10)\n",
        "  meridians = np.arange(0, 361,10)\n",
        "  pop = np.array([[float('nan') if num <= 0 else num for num in row] for row in population_reference])\n",
        "  for i in range(8):\n",
        "    plt.figure(figsize = (45, 30))\n",
        "    plt.subplot(3,2,1)\n",
        "    hm = Basemap(projection='lcc', resolution='c',\n",
        "            width=21E6, height=10E6, \n",
        "            lat_0=35, lon_0=145)\n",
        "    hm.shadedrelief(scale = 0.125)\n",
        "    hm.drawparallels(parallels, labels = [True, False, False, False])\n",
        "    hm.drawmeridians(meridians, lables = [False, False, True, True])\n",
        "    hm.pcolormesh(xx,yy, hour_exposure['data'][i], latlon=True, cmap= hour_exposure['cmapscheme'])\n",
        "    hm.drawcoastlines()\n",
        "    plt.clim(hour_exposure['cmapmin'], hour_exposure['cmapmax'])\n",
        "    plt.colorbar(label= hour_exposure['cmaplabel'])\n",
        "    plt.title(hour_exposure['label'] + ' - Phase %d' %(i+ 1))\n",
        "    \n",
        "    plt.subplot(3,2,2)\n",
        "    hm = Basemap(projection='lcc', resolution='c',\n",
        "            width=21E6, height=10E6, \n",
        "            lat_0=35, lon_0=145)\n",
        "    hm.shadedrelief(scale = 0.125)\n",
        "    hm.drawparallels(parallels, labels = [True, False, False, False])\n",
        "    hm.drawmeridians(meridians, lables = [False, False, True, True])\n",
        "    hm.pcolormesh(xx,yy, hour_anomalies['data'][i], latlon=True, cmap= hour_anomalies['cmapscheme'])\n",
        "    hm.drawcoastlines()\n",
        "    plt.clim(hour_anomalies['cmapmin'], hour_anomalies['cmapmax'])\n",
        "    plt.colorbar(label= hour_anomalies['cmaplabel'])\n",
        "    plt.title(hour_anomalies['label'] + ' - Phase %d' %(i+ 1)) \n",
        "    \n",
        "    plt.subplot(3,2,3)\n",
        "    hm.shadedrelief(scale = 0.125)\n",
        "    hm.drawparallels(parallels, labels = [True, False, False, False])\n",
        "    hm.drawmeridians(meridians, lables = [False, False, True, True])\n",
        "    hm.pcolormesh(xx,yy, pop_hours['data'][i], latlon=True, cmap= pop_hours['cmapscheme'])\n",
        "    hm.drawcoastlines()\n",
        "    plt.clim(pop_hours['cmapmin'], pop_hours['cmapmax'])\n",
        "    plt.colorbar(label= pop_hours['cmaplabel'])\n",
        "    plt.title(pop_hours['label'] + ' - Phase %d' %(i+ 1)) \n",
        "  \n",
        "    plt.subplot(3,2,4)\n",
        "    hm.shadedrelief(scale = 0.125)\n",
        "    hm.drawparallels(parallels, labels = [True, False, False, False])\n",
        "    hm.drawmeridians(meridians, lables = [False, False, True, True])\n",
        "    hm.pcolormesh(xx,yy, pop_hours_anomaly['data'][i], latlon=True, cmap= pop_hours_anomaly['cmapscheme'])\n",
        "    hm.drawcoastlines()\n",
        "    plt.clim(pop_hours_anomaly['cmapmin'], pop_hours_anomaly['cmapmax'])\n",
        "    plt.colorbar(label= pop_hours_anomaly['cmaplabel'])\n",
        "    plt.title(pop_hours_anomaly['label'] + ' - Phase %d' %(i+ 1))\n",
        "  \n",
        "    plt.subplot(3,2,5)\n",
        "    hm.shadedrelief(scale = 0.125)\n",
        "    hm.drawparallels(parallels, labels = [True, False, False, False])\n",
        "    hm.drawmeridians(meridians, lables = [False, False, True, True])\n",
        "    hm.pcolormesh(xx,yy, pop, latlon=True, cmap= 'YlGn')\n",
        "    hm.drawcoastlines()\n",
        "    plt.clim(np.nanmin(pop), 10000000)\n",
        "    plt.colorbar(label= 'People')\n",
        "    plt.title('Binned Population') \n",
        "\n",
        "    plt.subplot(3,2,6)\n",
        "    hm.shadedrelief(scale = 0.125)\n",
        "    hm.drawparallels(parallels, labels = [True, False, False, False])\n",
        "    hm.drawmeridians(meridians, lables = [False, False, True, True])\n",
        "    hm.pcolormesh(xx,yy, pop_hours_diff['data'][i], latlon=True, cmap= pop_hours_diff['cmapscheme'])\n",
        "    hm.drawcoastlines()\n",
        "    plt.clim(pop_hours_diff['cmapmin'], pop_hours_diff['cmapmax'])\n",
        "    plt.colorbar(label= pop_hours_diff['cmaplabel'])\n",
        "    plt.title(pop_hours_diff['label'] + ' - Phase %d' %(i+ 1))\n",
        "    \n",
        "import cv2\n",
        "import os\n",
        "from os.path import isfile, join\n",
        "# from IPython.display import HTML\n",
        "# figure out .avi / .mp4 and embedding within IPython file\n",
        "# desired: create function so that this can be called on certain files\n",
        "def create_video(videoname):\n",
        "  image_folder = \"/content/\"\n",
        "  video_name = videoname + '.mp4'\n",
        "\n",
        "  images = [img for img in sorted(os.listdir(image_folder)) if img.endswith(\".png\")]\n",
        "  frame = cv2.imread(os.path.join(image_folder, images[0]))\n",
        "  height, width, layers = frame.shape\n",
        "\n",
        "  video = cv2.VideoWriter(video_name, 0, 1, (width,height))\n",
        "\n",
        "  for image in images:\n",
        "      video.write(cv2.imread(os.path.join(image_folder, image)))\n",
        "\n",
        "  cv2.destroyAllWindows()\n",
        "  video.release()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6VRKIHJZJed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_paths = [refine_data(gather_data(True, True, i , True)) for i in range(6)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
        "all_paths = [hour_binning(data) for data in all_paths]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66sIJSGsZ5wB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "for category in range(len(all_paths)):\n",
        "  hour_set_plots = data_dict(all_paths[category],'Hours Exposed', None, None, 'rainbow', np.nanmin(np.array(all_paths[category])), np.nanmax(np.array(all_paths[category])), 'Hours') \n",
        "\n",
        "  hour_anom = popxhours_anomaly(all_paths[category], True)\n",
        "  set_0 = abs(np.nanmax(np.array(hour_anom))) if abs(np.nanmax(np.array(hour_anom))) > abs(np.nanmin(np.array(hour_anom))) else abs(np.nanmin(np.array(hour_anom)))\n",
        "  hour_anomaly_plots = data_dict(hour_anom, 'Hours Exposed Anomaly', None, None, 'RdBu', -set_0, set_0, 'Percent Difference From Mean')\n",
        "\n",
        "  binned_pop = population_binning(dataGTIFF)\n",
        "  all_data = popxhours(binned_pop, all_paths[category])\n",
        "  pop_hours_data = data_dict(all_data,'Population-Hours', None, None, 'rainbow', np.nanmin(np.array(all_data)), np.nanmax(np.array(all_data)), 'Person-Hours')  \n",
        "\n",
        "  anom = popxhours_anomaly(all_data, True)\n",
        "  set_0 = abs(np.nanmax(np.array(anom))) if abs(np.nanmax(np.array(anom))) > abs(np.nanmin(np.array(anom))) else abs(np.nanmin(np.array(anom)))\n",
        "  anomaly_dataset = data_dict(anom, 'Population Hour Anomaly', None, None, 'RdBu', -set_0, set_0, 'Percent Difference From Mean')\n",
        "\n",
        "  non_percentage = popxhours_anomaly(all_data, False)\n",
        "  limit_difference = abs(np.nanmax(np.array(non_percentage))) if abs(np.nanmax(np.array(non_percentage))) > abs(np.nanmin(np.array(non_percentage))) else abs(np.nanmin(np.array(non_percentage)))\n",
        "  diff_dataset = data_dict(non_percentage, 'Population Hour Anomaly', None, None, 'RdBu', -limit_difference, limit_difference, 'Difference From Mean')\n",
        "\n",
        "  hour_anomaly_plots['cmapmin'] = -np.nanpercentile(hour_anomaly_plots['data'], 98.75)\n",
        "  hour_anomaly_plots['cmapmax'] =  np.nanpercentile(hour_anomaly_plots['data'], 98.75)\n",
        "\n",
        "  anomaly_dataset['cmapmin']    = -np.nanpercentile(anomaly_dataset['data'], 98.75)\n",
        "  anomaly_dataset['cmapmax']    =  np.nanpercentile(anomaly_dataset['data'], 98.75)\n",
        "  pop_hours_data['cmapmin']     =  0\n",
        "  pop_hours_data['cmapmax']     =  np.nanpercentile(pop_hours_data['data'], 98.75)\n",
        "\n",
        "  diff_dataset['cmapmin']       = -np.nanpercentile(diff_dataset['data'], 97)\n",
        "  diff_dataset['cmapmax']       =  np.nanpercentile(diff_dataset['data'], 97)\n",
        "\n",
        "  create_maps_subplotted(binned_pop, hour_set_plots, hour_anomaly_plots, pop_hours_data, anomaly_dataset, diff_dataset)\n",
        "\n",
        "  figures=[manager.canvas.figure for manager in mpl._pylab_helpers.Gcf.get_all_fig_managers()]\n",
        "  for i, figure in enumerate(figures):\n",
        "    figure.savefig('suplots_%d.png' % i)\n",
        "\n",
        "  create_video('Category ' + str(category) + ' Maps')\n",
        "  mpl._pylab_helpers.Gcf.destroy_all()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJMt39mDh7ee",
        "colab_type": "code",
        "outputId": "65550d79-b2e7-4bbd-e66e-b83fcb539bde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "mpl._pylab_helpers.Gcf.get_all_fig_managers()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edmX63D1SvXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hour_set = hour_binning(gather_data())\n",
        "hour_set_plots = data_dict(hour_set,'Hours Exposed', None, None, 'rainbow', np.nanmin(np.array(hour_set)), np.nanmax(np.array(hour_set)), 'Hours')\n",
        "\n",
        "hour_anom = popxhours_anomaly(hour_set, True)\n",
        "set_0 = abs(np.nanmax(np.array(hour_anom))) if abs(np.nanmax(np.array(hour_anom))) > abs(np.nanmin(np.array(hour_anom))) else abs(np.nanmin(np.array(hour_anom)))\n",
        "hour_anomaly_plots = data_dict(hour_anom, 'Hours Exposed Anomaly', None, None, 'RdBu', -set_0, set_0, 'Percent Difference From Mean')\n",
        "\n",
        "binned_pop = population_binning(dataGTIFF)\n",
        "all_data = popxhours(binned_pop, hour_set)\n",
        "all_data = np.log10(all_data)\n",
        "pop_hours_data = data_dict(all_data,'Population-Hours', None, None, 'rainbow', np.nanmin(np.array(all_data)), np.nanmax(np.array(all_data)), 'Person-Hours')\n",
        "\n",
        "anom = popxhours_anomaly(all_data, True)\n",
        "set_0 = abs(np.nanmax(np.array(anom))) if abs(np.nanmax(np.array(anom))) > abs(np.nanmin(np.array(anom))) else abs(np.nanmin(np.array(anom)))\n",
        "anomaly_dataset = data_dict(anom, 'Population Hour Anomaly', None, None, 'RdBu', -set_0, set_0, 'Percent Difference From Mean')\n",
        "\n",
        "non_percentage = popxhours_anomaly(all_data, False)\n",
        "limit_difference = abs(np.nanmax(np.array(non_percentage))) if abs(np.nanmax(np.array(non_percentage))) > abs(np.nanmin(np.array(non_percentage))) else abs(np.nanmin(np.array(non_percentage)))\n",
        "diff_dataset = data_dict(non_percentage, 'Population Hour Anomaly', None, None, 'RdBu', -limit_difference, limit_difference, 'Difference From Mean')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPrf2q_2SwCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hour_anomaly_plots['cmapmin'] = -np.nanpercentile(hour_anomaly_plots['data'], 98.75)\n",
        "hour_anomaly_plots['cmapmax'] =  np.nanpercentile(hour_anomaly_plots['data'], 98.75)\n",
        "\n",
        "anomaly_dataset['cmapmin'] = -np.nanpercentile(anomaly_dataset['data'], 98.75)\n",
        "anomaly_dataset['cmapmax'] =  np.nanpercentile(anomaly_dataset['data'], 98.75)\n",
        "pop_hours_data['cmapmin'] =  0\n",
        "pop_hours_data['cmapmax'] =  np.nanpercentile(pop_hours_data['data'], 98.75)\n",
        "\n",
        "diff_dataset['cmapmin'] = -np.nanpercentile(diff_dataset['data'], 97)\n",
        "diff_dataset['cmapmax'] =  np.nanpercentile(diff_dataset['data'], 97)\n",
        "\n",
        "#diff anomaly dataset and houranomaly seem to be off?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YOPdbXQxJEZ",
        "colab_type": "text"
      },
      "source": [
        "### Data Variables\n",
        "* `binned pop`     - binned population grid\n",
        "* `hour_set_plots` - binned hours\n",
        "* `hour_anomaly_plots` - hour anomalies (phase hours - mean hours) / mean\n",
        "* `pop_hours_data` - binned hours * binned population grid\n",
        "* `anomaly_dataset` - population hour anomalies (same as hour_anomaly but focuses on population centers)\n",
        "* `diff_dataset` - raw difference (phase - mean)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud2ZIA0365wS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "create_maps_subplotted(binned_pop, hour_set_plots, hour_anomaly_plots, pop_hours_data, anomaly_dataset, diff_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-3UkVciPfwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "# create_maps_subplotted(binned_pop, hour_set_plots, hour_anomaly_plots, pop_hours_data, anomaly_dataset, diff_dataset)\n",
        "create_maps(pop_hours_data)\n",
        "#all lines (besides the last line) must be uncommented to create the video\n",
        "#in the next code block\n",
        "\n",
        "figures=[manager.canvas.figure\n",
        "       for manager in mpl._pylab_helpers.Gcf.get_all_fig_managers()]\n",
        "for i, figure in enumerate(figures):\n",
        "  figure.savefig('suplots_%d.png' % i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IvXk1nShYT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "create_video('6_figs')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}